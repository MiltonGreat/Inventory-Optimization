{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeac8c35-5ea2-45cd-aa51-a8b12878758c",
   "metadata": {},
   "source": [
    "# Inventory Optimization Using Reinforcement Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dffae91c-5b93-4263-a154-0668d6f73291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\milto\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\milto\\anaconda3\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\milto\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\milto\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\milto\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\milto\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\milto\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\milto\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\milto\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\milto\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\milto\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\milto\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\milto\\anaconda3\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\milto\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9256e274-b794-4bbd-abf0-64ff6049bb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OrderNumber Sales Channel WarehouseCode ProcuredDate  OrderDate  ShipDate  \\\n",
      "0  SO - 000101      In-Store  WARE-UHY1004     31/12/17 2018-05-31   14/6/18   \n",
      "1  SO - 000102        Online  WARE-NMK1003     31/12/17 2018-05-31   22/6/18   \n",
      "2  SO - 000103   Distributor  WARE-UHY1004     31/12/17 2018-05-31   21/6/18   \n",
      "3  SO - 000104     Wholesale  WARE-NMK1003     31/12/17 2018-05-31  2/6/2018   \n",
      "4  SO - 000105   Distributor  WARE-NMK1003    10/4/2018 2018-05-31   16/6/18   \n",
      "\n",
      "  DeliveryDate CurrencyCode  _SalesTeamID  _CustomerID  _StoreID  _ProductID  \\\n",
      "0   2018-06-19          USD             6           15       259          12   \n",
      "1          NaT          USD            14           20       196          27   \n",
      "2          NaT          USD            21           16       213          16   \n",
      "3          NaT          USD            28           48       107          23   \n",
      "4   2018-06-26          USD            22           49       111          26   \n",
      "\n",
      "   Order Quantity  Discount Applied  Unit Cost  Unit Price  LeadTime  \\\n",
      "0               5              0.08    1001.18      1963.1      19.0   \n",
      "1               3              0.08    3348.66      3939.6       NaN   \n",
      "2               1              0.05     781.22      1775.5       NaN   \n",
      "3               8              0.08    1464.69      2324.9       NaN   \n",
      "4               8              0.10    1476.14      1822.4      26.0   \n",
      "\n",
      "   TotalRevenue  TotalCost  \n",
      "0        9815.5    5005.90  \n",
      "1       11818.8   10045.98  \n",
      "2        1775.5     781.22  \n",
      "3       18599.2   11717.52  \n",
      "4       14579.2   11809.12  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Data Preprocessing\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'US_Regional_Sales_Data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Clean 'Unit Cost' and 'Unit Price' by removing $ signs and converting to float\n",
    "data['Unit Cost'] = data['Unit Cost'].replace(r'[\\$,]', '', regex=True).astype(float)  # Use raw string for regex\n",
    "data['Unit Price'] = data['Unit Price'].replace(r'[\\$,]', '', regex=True).astype(float)  # Use raw string for regex\n",
    "\n",
    "# Ensure there are no remaining NaN values in the 'Unit Cost' column after conversion\n",
    "data['Unit Cost'] = data['Unit Cost'].fillna(0)  # Use fillna without inplace=True\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "data['OrderDate'] = pd.to_datetime(data['OrderDate'], format='%d/%m/%y', errors='coerce')\n",
    "data['DeliveryDate'] = pd.to_datetime(data['DeliveryDate'], format='%d/%m/%y', errors='coerce')\n",
    "\n",
    "# Feature Engineering: Create additional features\n",
    "data['LeadTime'] = (data['DeliveryDate'] - data['OrderDate']).dt.days\n",
    "data['TotalRevenue'] = data['Order Quantity'] * data['Unit Price']\n",
    "data['TotalCost'] = data['Order Quantity'] * data['Unit Cost']\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm changes\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47993e7f-5fab-4cc1-ab47-a59ea0abc89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Warehouse Environment Definition\n",
    "# ---------------------------------------------\n",
    "\n",
    "class WarehouseEnv:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.current_stock = {product_id: 0 for product_id in self.data['_ProductID'].unique()}\n",
    "        self.action_space = [-1, 0, 1]  # -1: reduce stock, 0: maintain stock, 1: increase stock\n",
    "        self.max_stock = 100  # Maximum stock level to prevent overstocking\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
    "        self.current_stock = {product_id: 0 for product_id in self.data['_ProductID'].unique()}\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Get the current stock state.\"\"\"\n",
    "        return np.array([self.current_stock[product_id] for product_id in self.current_stock])\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return the new state, reward, and done flag.\"\"\"\n",
    "        for product_id in self.current_stock:\n",
    "            if action == 1 and self.current_stock[product_id] < self.max_stock:\n",
    "                self.current_stock[product_id] += 1  # Increase stock\n",
    "            elif action == -1 and self.current_stock[product_id] > 0:\n",
    "                self.current_stock[product_id] -= 1  # Reduce stock\n",
    "\n",
    "        reward = self.calculate_reward()\n",
    "        done = False  # No terminal state for now\n",
    "        return self.get_state(), reward, done, {}\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        \"\"\"Calculate the reward based on stock levels, carrying costs, and stockout penalties.\"\"\"\n",
    "        # Ensure that 'Unit Cost' is treated as a float\n",
    "        carrying_costs = sum(\n",
    "            stock * float(self.data.loc[self.data['_ProductID'] == product_id, 'Unit Cost'].values[0])\n",
    "            for product_id, stock in self.current_stock.items()\n",
    "        )\n",
    "\n",
    "        # Stockout penalty (reduced penalty for stockouts to avoid overwhelming negative rewards)\n",
    "        stockout_penalty = -sum(1 for stock in self.current_stock.values() if stock == 0) * 20\n",
    "\n",
    "        # Reward for maintaining balanced stock levels (between 10 and 30 units)\n",
    "        optimal_stock_reward = sum(1 for stock in self.current_stock.values() if 10 <= stock <= 30) * 30\n",
    "\n",
    "        # Penalty for overstocking beyond 50 units\n",
    "        overstock_penalty = -sum(1 for stock in self.current_stock.values() if stock > 50) * 10\n",
    "\n",
    "        # Final reward calculation\n",
    "        reward = -carrying_costs + stockout_penalty + optimal_stock_reward + overstock_penalty\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "571846c9-07cf-4f59-94a7-b296b80e392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# DQN Agent Definition\n",
    "# ---------------------------------------------\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.98  # Slightly increased gamma to consider future rewards more\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995  # Slower decay for more exploration\n",
    "        self.learning_rate = 0.0005  # Reduced learning rate for better optimization\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, self.action_size)\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model(torch.FloatTensor(state))  # Predict Q-values\n",
    "        return torch.argmax(act_values).item()\n",
    "\n",
    "    def replay(self, batch_size=64):  # Increased batch size for more diverse training data\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * torch.max(self.model(torch.FloatTensor(next_state))).item()\n",
    "            target_f = self.model(torch.FloatTensor(state))\n",
    "            target_f[action] = target\n",
    "\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "            self.model.zero_grad()\n",
    "            loss = criterion(self.model(torch.FloatTensor(state)), target_f)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7098f835-8bed-428b-8604-3b1faf07a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properly clean the 'Unit Cost' and 'Unit Price' columns by removing commas and dollar signs\n",
    "data['Unit Cost'] = data['Unit Cost'].replace(r'[\\$,]', '', regex=True).astype(float)\n",
    "data['Unit Price'] = data['Unit Price'].replace(r'[\\$,]', '', regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60656af-310a-4d13-a171-f70e8d72e917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unit Cost  Unit Price\n",
      "0    1001.18      1963.1\n",
      "1    3348.66      3939.6\n",
      "2     781.22      1775.5\n",
      "3    1464.69      2324.9\n",
      "4    1476.14      1822.4\n"
     ]
    }
   ],
   "source": [
    "print(data[['Unit Cost', 'Unit Price']].head())  # This should print numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aad31ab-a11d-43ea-9415-cbde9306ac2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/100, Total Reward: -2823463999.92, Epsilon: 0.99\n",
      "Episode: 2/100, Total Reward: -2777443389.76, Epsilon: 0.99\n",
      "Episode: 3/100, Total Reward: -2754627118.48, Epsilon: 0.99\n",
      "Episode: 4/100, Total Reward: -2738833694.48, Epsilon: 0.98\n",
      "Episode: 5/100, Total Reward: -2848659436.80, Epsilon: 0.98\n",
      "Episode: 6/100, Total Reward: -2752773230.00, Epsilon: 0.97\n",
      "Episode: 7/100, Total Reward: -2738918734.00, Epsilon: 0.97\n",
      "Episode: 8/100, Total Reward: -2757476571.20, Epsilon: 0.96\n",
      "Episode: 9/100, Total Reward: -2708909536.40, Epsilon: 0.96\n",
      "Episode: 10/100, Total Reward: -2638894822.16, Epsilon: 0.95\n",
      "Episode: 11/100, Total Reward: -2597647113.20, Epsilon: 0.95\n",
      "Episode: 12/100, Total Reward: -2496203880.00, Epsilon: 0.94\n",
      "Episode: 13/100, Total Reward: -2536694723.76, Epsilon: 0.94\n",
      "Episode: 14/100, Total Reward: -2808541150.64, Epsilon: 0.93\n",
      "Episode: 15/100, Total Reward: -2812761075.20, Epsilon: 0.93\n",
      "Episode: 16/100, Total Reward: -2667756736.96, Epsilon: 0.92\n",
      "Episode: 17/100, Total Reward: -2707597344.56, Epsilon: 0.92\n",
      "Episode: 18/100, Total Reward: -2789838614.40, Epsilon: 0.91\n",
      "Episode: 19/100, Total Reward: -2600291236.88, Epsilon: 0.91\n",
      "Episode: 20/100, Total Reward: -2614095912.88, Epsilon: 0.90\n",
      "Episode: 21/100, Total Reward: -2622017064.88, Epsilon: 0.90\n",
      "Episode: 22/100, Total Reward: -2507010174.24, Epsilon: 0.90\n",
      "Episode: 23/100, Total Reward: -2583335960.08, Epsilon: 0.89\n",
      "Episode: 24/100, Total Reward: -2472035936.88, Epsilon: 0.89\n",
      "Episode: 25/100, Total Reward: -2485421055.28, Epsilon: 0.88\n",
      "Episode: 26/100, Total Reward: -2692730864.80, Epsilon: 0.88\n",
      "Episode: 27/100, Total Reward: -2482064215.92, Epsilon: 0.87\n",
      "Episode: 28/100, Total Reward: -2730423485.84, Epsilon: 0.87\n",
      "Episode: 29/100, Total Reward: -2757332342.16, Epsilon: 0.86\n",
      "Episode: 30/100, Total Reward: -2444633293.44, Epsilon: 0.86\n",
      "Episode: 31/100, Total Reward: -2435798357.20, Epsilon: 0.86\n",
      "Episode: 32/100, Total Reward: -2643407904.80, Epsilon: 0.85\n",
      "Episode: 33/100, Total Reward: -2380329234.64, Epsilon: 0.85\n",
      "Episode: 34/100, Total Reward: -2404070100.16, Epsilon: 0.84\n",
      "Episode: 35/100, Total Reward: -2618709545.04, Epsilon: 0.84\n",
      "Episode: 36/100, Total Reward: -2692918803.84, Epsilon: 0.83\n",
      "Episode: 37/100, Total Reward: -2411784043.12, Epsilon: 0.83\n",
      "Episode: 38/100, Total Reward: -2543389162.96, Epsilon: 0.83\n",
      "Episode: 39/100, Total Reward: -2415810858.64, Epsilon: 0.82\n",
      "Episode: 40/100, Total Reward: -2535207752.88, Epsilon: 0.82\n",
      "Episode: 41/100, Total Reward: -2528846391.28, Epsilon: 0.81\n",
      "Episode: 42/100, Total Reward: -2335457317.28, Epsilon: 0.81\n",
      "Episode: 43/100, Total Reward: -2517082602.80, Epsilon: 0.81\n",
      "Episode: 44/100, Total Reward: -2682027000.08, Epsilon: 0.80\n",
      "Episode: 45/100, Total Reward: -2357180325.28, Epsilon: 0.80\n",
      "Episode: 46/100, Total Reward: -2571106584.48, Epsilon: 0.79\n",
      "Episode: 47/100, Total Reward: -2553424022.00, Epsilon: 0.79\n",
      "Episode: 48/100, Total Reward: -2621560407.76, Epsilon: 0.79\n",
      "Episode: 49/100, Total Reward: -2600872883.52, Epsilon: 0.78\n",
      "Episode: 50/100, Total Reward: -2276161037.76, Epsilon: 0.78\n",
      "Episode: 51/100, Total Reward: -2273186156.00, Epsilon: 0.77\n",
      "Episode: 52/100, Total Reward: -2645231743.76, Epsilon: 0.77\n",
      "Episode: 53/100, Total Reward: -2367584921.92, Epsilon: 0.77\n",
      "Episode: 54/100, Total Reward: -2305120181.60, Epsilon: 0.76\n",
      "Episode: 55/100, Total Reward: -2455864315.28, Epsilon: 0.76\n",
      "Episode: 56/100, Total Reward: -2305782637.76, Epsilon: 0.76\n",
      "Episode: 57/100, Total Reward: -2249195341.92, Epsilon: 0.75\n",
      "Episode: 58/100, Total Reward: -1930381150.64, Epsilon: 0.75\n",
      "Episode: 59/100, Total Reward: -2173321983.20, Epsilon: 0.74\n",
      "Episode: 60/100, Total Reward: -2408409813.76, Epsilon: 0.74\n",
      "Episode: 61/100, Total Reward: -2175997126.88, Epsilon: 0.74\n",
      "Episode: 62/100, Total Reward: -2296250025.84, Epsilon: 0.73\n",
      "Episode: 63/100, Total Reward: -2620371294.96, Epsilon: 0.73\n",
      "Episode: 64/100, Total Reward: -2144374119.36, Epsilon: 0.73\n",
      "Episode: 65/100, Total Reward: -2409944254.64, Epsilon: 0.72\n",
      "Episode: 66/100, Total Reward: -1984601500.88, Epsilon: 0.72\n",
      "Episode: 67/100, Total Reward: -2229146773.84, Epsilon: 0.71\n",
      "Episode: 68/100, Total Reward: -2308061253.84, Epsilon: 0.71\n",
      "Episode: 69/100, Total Reward: -2282768587.92, Epsilon: 0.71\n",
      "Episode: 70/100, Total Reward: -2277912997.20, Epsilon: 0.70\n",
      "Episode: 71/100, Total Reward: -2223223299.84, Epsilon: 0.70\n",
      "Episode: 72/100, Total Reward: -2185240810.72, Epsilon: 0.70\n",
      "Episode: 73/100, Total Reward: -2406258506.72, Epsilon: 0.69\n",
      "Episode: 74/100, Total Reward: -3201524681.20, Epsilon: 0.69\n",
      "Episode: 75/100, Total Reward: -2165098273.12, Epsilon: 0.69\n",
      "Episode: 76/100, Total Reward: -3219840089.84, Epsilon: 0.68\n",
      "Episode: 77/100, Total Reward: -3213109930.64, Epsilon: 0.68\n",
      "Episode: 78/100, Total Reward: -3274225819.12, Epsilon: 0.68\n",
      "Episode: 79/100, Total Reward: -3262608609.68, Epsilon: 0.67\n",
      "Episode: 80/100, Total Reward: -3372264742.96, Epsilon: 0.67\n",
      "Episode: 81/100, Total Reward: -2454061626.32, Epsilon: 0.67\n",
      "Episode: 82/100, Total Reward: -2010133305.36, Epsilon: 0.66\n",
      "Episode: 83/100, Total Reward: -2119095522.96, Epsilon: 0.66\n",
      "Episode: 84/100, Total Reward: -1807966134.64, Epsilon: 0.66\n",
      "Episode: 85/100, Total Reward: -1956122984.16, Epsilon: 0.65\n",
      "Episode: 86/100, Total Reward: -2450393268.40, Epsilon: 0.65\n",
      "Episode: 87/100, Total Reward: -2015057485.60, Epsilon: 0.65\n",
      "Episode: 88/100, Total Reward: -3280776029.28, Epsilon: 0.64\n",
      "Episode: 89/100, Total Reward: -3352760221.52, Epsilon: 0.64\n",
      "Episode: 90/100, Total Reward: -3262304171.60, Epsilon: 0.64\n",
      "Episode: 91/100, Total Reward: -3341714318.72, Epsilon: 0.63\n",
      "Episode: 92/100, Total Reward: -2008048268.32, Epsilon: 0.63\n",
      "Episode: 93/100, Total Reward: -2372496882.16, Epsilon: 0.63\n",
      "Episode: 94/100, Total Reward: -1822810524.40, Epsilon: 0.62\n",
      "Episode: 95/100, Total Reward: -2257242392.96, Epsilon: 0.62\n",
      "Episode: 96/100, Total Reward: -1927713526.96, Epsilon: 0.62\n",
      "Episode: 97/100, Total Reward: -3331930348.24, Epsilon: 0.61\n",
      "Episode: 98/100, Total Reward: -3270109734.08, Epsilon: 0.61\n",
      "Episode: 99/100, Total Reward: -3189278855.60, Epsilon: 0.61\n",
      "Episode: 100/100, Total Reward: -3284950363.84, Epsilon: 0.61\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Training the DQN Agent\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Example usage\n",
    "env = WarehouseEnv(data)\n",
    "state_size = len(env.get_state())\n",
    "action_size = len(env.action_space)\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Training loop\n",
    "episodes = 100  # Can increase this if needed\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward, new_state, done)\n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    if len(agent.memory) > 32:\n",
    "        agent.replay(32)\n",
    "    agent.update_epsilon()\n",
    "    print(f\"Episode: {e+1}/{episodes}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f678f-d8ff-4f35-b3ce-095025731a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward during test run: -2030285242.96\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Testing the DQN Agent\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Test the agent after training\n",
    "test_reward = 0\n",
    "state = env.reset()\n",
    "for time in range(500):\n",
    "    action = agent.act(state)\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    test_reward += reward\n",
    "    state = new_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"Total reward during test run: {test_reward:.2f}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Visualize Training Progress\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Optional: Add a visualization for rewards during training (example for better analysis)\n",
    "rewards = []\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward, new_state, done)\n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(total_reward)\n",
    "    if len(agent.memory) > 32:\n",
    "        agent.replay(32)\n",
    "    agent.update_epsilon()\n",
    "\n",
    "# Plot rewards over episodes\n",
    "plt.plot(range(episodes), rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward over Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d145b-6a23-4174-b8cd-c70d44a4447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Evaluate Model Performance\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Load the model before evaluation if you've saved it\n",
    "agent.model.load_state_dict(torch.load('dqn_agent_model.pth'))\n",
    "\n",
    "# Example evaluation: Use the final model to simulate a test run\n",
    "state = env.reset()  # Reset the environment\n",
    "test_reward = 0\n",
    "\n",
    "# Run the test loop\n",
    "for time in range(500):\n",
    "    action = agent.act(state)  # Use the trained agent to choose an action\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    test_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"Total reward during test run: {test_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
